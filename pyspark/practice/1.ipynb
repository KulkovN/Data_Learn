{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 'https://gist.githubusercontent.com/jwalsh/ce1dc0436aba5b7a5c9666f47fa5a380/raw/5ce3854392b43ff97907112d344fc008229b0445/titanic.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import toPandas\n",
    "from datetime import datetime, date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализация спарка\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        Row(a=1, b=2, c='str1', d=date(2022, 1, 1), e=datetime(2022, 1, 1, 12, 0)),\n",
    "        Row(a=3, b=3, c='str1', d=date(2022, 1, 1), e=datetime(2022, 1, 1, 12, 0)),\n",
    "        Row(a=4, b=5, c='str1', d=date(2022, 1, 1), e=datetime(2022, 1, 1, 12, 0)),\n",
    "    ]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+-------+----------+-------------------+\n",
       "|  a|  b|      c|         d|                  e|\n",
       "+---+---+-------+----------+-------------------+\n",
       "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
       "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
       "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
       "+---+---+-------+----------+-------------------+"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создайте PySpark DataFrame из pandas DataFrame\n",
    "panda_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(panda_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+-------+----------+-------------------+\n",
       "|  a|  b|      c|         d|                  e|\n",
       "+---+---+-------+----------+-------------------+\n",
       "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
       "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
       "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
       "+---+---+-------+----------+-------------------+"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создайте PySpark DataFrame из RDD, состоящего из списка кортежей.\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with schema\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создать дф на своих данных\n",
    "from pathlib import Path\n",
    "path = f'{Path.home()}/Desktop/learning/learn_spark/Data_learn'\n",
    "df = spark.read.format('csv').options(header=False)\\\n",
    "     .load(f'{path}/pandas/titanic.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='0', _c1='1', _c2='0', _c3='3', _c4='Braund', _c5=' Mr. Owen Harris', _c6='male', _c7='22.0', _c8='1', _c9='0', _c10='A/5 21171', _c11='7.25', _c12=None, _c13='S')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### просмотр данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------\n",
      " _c0  | 0                \n",
      " _c1  | 1                \n",
      " _c2  | 0                \n",
      " _c3  | 3                \n",
      " _c4  | Braund           \n",
      " _c5  |  Mr. Owen Harris \n",
      " _c6  | male             \n",
      " _c7  | 22.0             \n",
      " _c8  | 1                \n",
      " _c9  | 0                \n",
      " _c10 | A/5 21171        \n",
      " _c11 | 7.25             \n",
      " _c12 | null             \n",
      " _c13 | S                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th><th>_c6</th><th>_c7</th><th>_c8</th><th>_c9</th><th>_c10</th><th>_c11</th><th>_c12</th><th>_c13</th></tr>\n",
       "<tr><td>0</td><td>1</td><td>0</td><td>3</td><td>Braund</td><td> Mr. Owen Harris</td><td>male</td><td>22.0</td><td>1</td><td>0</td><td>A/5 21171</td><td>7.25</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>1</td><td>2</td><td>1</td><td>1</td><td>Cumings</td><td> Mrs. John Bradle...</td><td>female</td><td>38.0</td><td>1</td><td>0</td><td>PC 17599</td><td>71.2833</td><td>C85</td><td>C</td></tr>\n",
       "<tr><td>2</td><td>3</td><td>1</td><td>3</td><td>Heikkinen</td><td> Miss. Laina</td><td>female</td><td>26.0</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td>7.925</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>3</td><td>4</td><td>1</td><td>1</td><td>Futrelle</td><td> Mrs. Jacques Hea...</td><td>female</td><td>35.0</td><td>1</td><td>0</td><td>113803</td><td>53.1</td><td>C123</td><td>S</td></tr>\n",
       "<tr><td>4</td><td>5</td><td>0</td><td>3</td><td>Allen</td><td> Mr. William Henry</td><td>male</td><td>35.0</td><td>0</td><td>0</td><td>373450</td><td>8.05</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>5</td><td>6</td><td>0</td><td>3</td><td>Moran</td><td> Mr. James</td><td>male</td><td>null</td><td>0</td><td>0</td><td>330877</td><td>8.4583</td><td>null</td><td>Q</td></tr>\n",
       "<tr><td>6</td><td>7</td><td>0</td><td>1</td><td>McCarthy</td><td> Mr. Timothy J</td><td>male</td><td>54.0</td><td>0</td><td>0</td><td>17463</td><td>51.8625</td><td>E46</td><td>S</td></tr>\n",
       "<tr><td>7</td><td>8</td><td>0</td><td>3</td><td>Palsson</td><td> Master. Gosta Le...</td><td>male</td><td>2.0</td><td>3</td><td>1</td><td>349909</td><td>21.075</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>8</td><td>9</td><td>1</td><td>3</td><td>Johnson</td><td> Mrs. Oscar W (El...</td><td>female</td><td>27.0</td><td>0</td><td>2</td><td>347742</td><td>11.1333</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>9</td><td>10</td><td>1</td><td>2</td><td>Nasser</td><td> Mrs. Nicholas (A...</td><td>female</td><td>14.0</td><td>1</td><td>0</td><td>237736</td><td>30.0708</td><td>null</td><td>C</td></tr>\n",
       "<tr><td>10</td><td>11</td><td>1</td><td>3</td><td>Sandstrom</td><td> Miss. Marguerite...</td><td>female</td><td>4.0</td><td>1</td><td>1</td><td>PP 9549</td><td>16.7</td><td>G6</td><td>S</td></tr>\n",
       "<tr><td>11</td><td>12</td><td>1</td><td>1</td><td>Bonnell</td><td> Miss. Elizabeth</td><td>female</td><td>58.0</td><td>0</td><td>0</td><td>113783</td><td>26.55</td><td>C103</td><td>S</td></tr>\n",
       "<tr><td>12</td><td>13</td><td>0</td><td>3</td><td>Saundercock</td><td> Mr. William Henry</td><td>male</td><td>20.0</td><td>0</td><td>0</td><td>A/5. 2151</td><td>8.05</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>13</td><td>14</td><td>0</td><td>3</td><td>Andersson</td><td> Mr. Anders Johan</td><td>male</td><td>39.0</td><td>1</td><td>5</td><td>347082</td><td>31.275</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>14</td><td>15</td><td>0</td><td>3</td><td>Vestrom</td><td> Miss. Hulda Aman...</td><td>female</td><td>14.0</td><td>0</td><td>0</td><td>350406</td><td>7.8542</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>15</td><td>16</td><td>1</td><td>2</td><td>Hewlett</td><td> Mrs. (Mary D Kin...</td><td>female</td><td>55.0</td><td>0</td><td>0</td><td>248706</td><td>16.0</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>16</td><td>17</td><td>0</td><td>3</td><td>Rice</td><td> Master. Eugene</td><td>male</td><td>2.0</td><td>4</td><td>1</td><td>382652</td><td>29.125</td><td>null</td><td>Q</td></tr>\n",
       "<tr><td>17</td><td>18</td><td>1</td><td>2</td><td>Williams</td><td> Mr. Charles Eugene</td><td>male</td><td>null</td><td>0</td><td>0</td><td>244373</td><td>13.0</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>18</td><td>19</td><td>0</td><td>3</td><td>Vander Planke</td><td> Mrs. Julius (Eme...</td><td>female</td><td>31.0</td><td>1</td><td>0</td><td>345763</td><td>18.0</td><td>null</td><td>S</td></tr>\n",
       "<tr><td>19</td><td>20</td><td>1</td><td>3</td><td>Masselmani</td><td> Mrs. Fatima</td><td>female</td><td>null</td><td>0</td><td>0</td><td>2649</td><td>7.225</td><td>null</td><td>C</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Кроме того, вы можете включить spark.sql.repl.eagerEval.enabled настройку для быстрой оценки PySpark DataFrame \n",
    "# в таких ноутбуках, как Jupyter. Количество отображаемых\n",
    "#  строк можно контролировать с помощью spark.sql.repl.eagerEval.maxNumRows конфигурации.\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13']\n"
     ]
    }
   ],
   "source": [
    "# имена столбов можно увидеть\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+\n",
      "|summary|                _c2|   _c6|\n",
      "+-------+-------------------+------+\n",
      "|  count|                156|   156|\n",
      "|   mean|0.34615384615384615|  null|\n",
      "| stddev|0.47727514420450834|  null|\n",
      "|    min|                  0|female|\n",
      "|    max|                  1|  male|\n",
      "+-------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Сводка по ДФ\n",
    "df.select('_c2','_c6').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='0', _c1='1', _c2='0', _c3='3', _c4='Braund', _c5=' Mr. Owen Harris', _c6='male', _c7='22.0', _c8='1', _c9='0', _c10='A/5 21171', _c11='7.25', _c12=None, _c13='S')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame.collect() собирает распределенные данные на стороне драйвера как локальные данные в Python.\n",
    "# Обратите внимание, что это может вызвать ошибку нехватки памяти, когда набор данных слишком велик, \n",
    "# чтобы поместиться на стороне драйвера, поскольку он собирает все данные от исполнителей на стороне драйвера.\n",
    "\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----\n",
      " _c4 | Braund \n",
      "only showing top 1 row\n",
      "\n",
      "+---+---+---+---+---------+--------------------+------+----+---+---+----------------+-------+----+----+\n",
      "|_c0|_c1|_c2|_c3|      _c4|                 _c5|   _c6| _c7|_c8|_c9|            _c10|   _c11|_c12|_c13|\n",
      "+---+---+---+---+---------+--------------------+------+----+---+---+----------------+-------+----+----+\n",
      "|  0|  1|  0|  3|   Braund|     Mr. Owen Harris|  male|22.0|  1|  0|       A/5 21171|   7.25|null|   S|\n",
      "|  1|  2|  1|  1|  Cumings| Mrs. John Bradle...|female|38.0|  1|  0|        PC 17599|71.2833| C85|   C|\n",
      "|  2|  3|  1|  3|Heikkinen|         Miss. Laina|female|26.0|  0|  0|STON/O2. 3101282|  7.925|null|   S|\n",
      "|  3|  4|  1|  1| Futrelle| Mrs. Jacques Hea...|female|35.0|  1|  0|          113803|   53.1|C123|   S|\n",
      "|  4|  5|  0|  3|    Allen|   Mr. William Henry|  male|35.0|  0|  0|          373450|   8.05|null|   S|\n",
      "+---+---+---+---+---------+--------------------+------+----+---+---+----------------+-------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df._c4).show(1, vertical=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---------+--------------------+------+----+---+---+----------------+-------+----+----+--------------------+\n",
      "|_c0|_c1|_c2|_c3|      _c4|                 _c5|   _c6| _c7|_c8|_c9|            _c10|   _c11|_c12|_c13|             new_col|\n",
      "+---+---+---+---+---------+--------------------+------+----+---+---+----------------+-------+----+----+--------------------+\n",
      "|  0|  1|  0|  3|   Braund|     Mr. Owen Harris|  male|22.0|  1|  0|       A/5 21171|   7.25|null|   S|     MR. OWEN HARRIS|\n",
      "|  1|  2|  1|  1|  Cumings| Mrs. John Bradle...|female|38.0|  1|  0|        PC 17599|71.2833| C85|   C| MRS. JOHN BRADLE...|\n",
      "|  2|  3|  1|  3|Heikkinen|         Miss. Laina|female|26.0|  0|  0|STON/O2. 3101282|  7.925|null|   S|         MISS. LAINA|\n",
      "|  3|  4|  1|  1| Futrelle| Mrs. Jacques Hea...|female|35.0|  1|  0|          113803|   53.1|C123|   S| MRS. JACQUES HEA...|\n",
      "|  4|  5|  0|  3|    Allen|   Mr. William Henry|  male|35.0|  0|  0|          373450|   8.05|null|   S|   MR. WILLIAM HENRY|\n",
      "+---+---+---+---+---------+--------------------+------+----+---+---+----------------+-------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# докинем еще один столб\n",
    "from pyspark.sql.functions import upper\n",
    "df.withColumn('new_col', upper(df._c5)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# присвоение имен колонкам через новый документы\n",
    "headers = [\n",
    "    'df_id',\n",
    "    'PassengerId',\n",
    "    'Survived',\n",
    "    'Pclass',\n",
    "    'Lname',\n",
    "    'Name',\n",
    "    'Sex',\n",
    "    'Age',\n",
    "    'SibSp',\n",
    "    'Parch',\n",
    "    'Ticket',\n",
    "    'Fare',\n",
    "    'Cabin',\n",
    "    'Embarked'\n",
    "]\n",
    "\n",
    "pd_df = df.toPandas()\n",
    "pd_df.columns = headers\n",
    "pd_df.to_csv('for_spark_titanic.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|    Lname|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+---------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|   Braund|     Mr. Owen Harris|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|  Cumings| Mrs. John Bradle...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen|         Miss. Laina|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1| Futrelle| Mrs. Jacques Hea...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|    Allen|   Mr. William Henry|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran|           Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1| McCarthy|       Mr. Timothy J|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|  Palsson| Master. Gosta Le...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|  Johnson| Mrs. Oscar W (El...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|   Nasser| Mrs. Nicholas (A...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "+-----------+--------+------+---------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').options(header=True) .load('for_spark_titanic.csv')\n",
    "# через распаковку сносим не нужные столбы\n",
    "tables = ['_c0', 'df_id']\n",
    "df = df.drop(*tables)\n",
    "\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(PassengerId,StringType,true),\n",
       " StructField(Survived,StringType,true),\n",
       " StructField(Pclass,StringType,true),\n",
       " StructField(Lname,StringType,true),\n",
       " StructField(Name,StringType,true),\n",
       " StructField(Sex,StringType,true),\n",
       " StructField(Age,StringType,true),\n",
       " StructField(SibSp,StringType,true),\n",
       " StructField(Parch,StringType,true),\n",
       " StructField(Ticket,StringType,true),\n",
       " StructField(Fare,StringType,true),\n",
       " StructField(Cabin,StringType,true),\n",
       " StructField(Embarked,StringType,true)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Филтьтрация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.Pclass == 3).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Survived', 'Pclass', 'Lname', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       0|  102|\n",
      "|       1|   54|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.schema.names)\n",
    "df.groupby('Survived').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с SQL \n",
    "DataFrame и Spark SQL используют один и тот же механизм выполнения, поэтому их можно беспрепятственно использовать взаимозаменяемо. Например, вы можете зарегистрировать DataFrame как таблицу и легко запустить SQL, как показано ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+--------+\n",
      "|        Lname|                Name|PassengerId|Survived|\n",
      "+-------------+--------------------+-----------+--------+\n",
      "|      Cumings| Mrs. John Bradle...|          2|       1|\n",
      "|    Heikkinen|         Miss. Laina|          3|       1|\n",
      "|     Futrelle| Mrs. Jacques Hea...|          4|       1|\n",
      "|      Johnson| Mrs. Oscar W (El...|          9|       1|\n",
      "|       Nasser| Mrs. Nicholas (A...|         10|       1|\n",
      "|    Sandstrom| Miss. Marguerite...|         11|       1|\n",
      "|      Bonnell|     Miss. Elizabeth|         12|       1|\n",
      "|      Hewlett| Mrs. (Mary D Kin...|         16|       1|\n",
      "|     Williams|  Mr. Charles Eugene|         18|       1|\n",
      "|   Masselmani|         Mrs. Fatima|         20|       1|\n",
      "|      Beesley|        Mr. Lawrence|         22|       1|\n",
      "|      McGowan|          Miss. Anna|         23|       1|\n",
      "|       Sloper| Mr. William Thom...|         24|       1|\n",
      "|      Asplund| Mrs. Carl Oscar ...|         26|       1|\n",
      "|      O'Dwyer|         Miss. Ellen|         29|       1|\n",
      "|      Spencer| Mrs. William Aug...|         32|       1|\n",
      "|        Glynn|   Miss. Mary Agatha|         33|       1|\n",
      "|        Mamee|           Mr. Hanna|         37|       1|\n",
      "|Nicola-Yarred|        Miss. Jamila|         40|       1|\n",
      "|      Laroche| Miss. Simonne Ma...|         44|       1|\n",
      "+-------------+--------------------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('titanic')\n",
    "quare = spark.sql('SELECT Lname, Name, PassengerId, Survived FROM titanic\\\n",
    "    WHERE Survived = 1 ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(PassengerId)|\n",
      "+------------------+\n",
      "|                54|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(PassengerId) from titanic where Survived = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+\n",
      "|PassengerId|     Lname|     Lname|\n",
      "+-----------+----------+----------+\n",
      "|          2|   Cumings|   Cumings|\n",
      "|          3| Heikkinen| Heikkinen|\n",
      "|          4|  Futrelle|  Futrelle|\n",
      "|          9|   Johnson|   Johnson|\n",
      "|         10|    Nasser|    Nasser|\n",
      "|         11| Sandstrom| Sandstrom|\n",
      "|         12|   Bonnell|   Bonnell|\n",
      "|         16|   Hewlett|   Hewlett|\n",
      "|         18|  Williams|  Williams|\n",
      "|         20|Masselmani|Masselmani|\n",
      "+-----------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# теперь тоже самое, но уже используя pysqark\n",
    "\n",
    "df.filter(F.col('Survived') == 1)\\\n",
    "    .select(F.col(\"PassengerId\"), F.col('Lname'), F.col('Lname')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fea05b1042538852277a77d2ef3e98d1acfa997b2fb8da2589056aa4b3964b97"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('py_environ': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
